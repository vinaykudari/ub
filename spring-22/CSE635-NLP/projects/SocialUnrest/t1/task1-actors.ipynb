{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |################################| 325 kB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |################################| 4.0 MB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |################################| 1.5 MB 29.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wandb\n",
      "  Downloading wandb-0.12.14-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |################################| 1.8 MB 21.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-0.6.2-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |################################| 65 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.2-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |################################| 22.2 MB 409 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |################################| 133 kB 32.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |################################| 86 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[K     |################################| 211 kB 42.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from datasets) (20.4)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[K     |################################| 78 kB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=5.0.0\n",
      "  Downloading pyarrow-6.0.1-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "\u001b[K     |################################| 25.6 MB 22.9 MB/s eta 0:00:01     |######################          | 18.1 MB 17.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py36-none-any.whl (106 kB)\n",
      "\u001b[K     |################################| 106 kB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |################################| 9.5 MB 16.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |################################| 1.1 MB 29.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.17.0-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |################################| 67 kB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.25.0)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (603 kB)\n",
      "\u001b[K     |################################| 603 kB 27.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2020.11.13)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |################################| 6.6 MB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from rouge-score) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from rouge-score) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
      "Collecting psutil>=5.0.0\n",
      "  Downloading psutil-5.9.0-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (279 kB)\n",
      "\u001b[K     |################################| 279 kB 27.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.2.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.5.10-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |################################| 144 kB 30.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
      "\u001b[K     |################################| 170 kB 33.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from accelerate) (1.6.0)\n",
      "Collecting scipy>=0.19.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |################################| 25.9 MB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->datasets) (2.4.7)\n",
      "Collecting importlib-resources; python_version < \"3.7\"\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "\u001b[K     |################################| 503 kB 29.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (270 kB)\n",
      "\u001b[K     |################################| 270 kB 39.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna-ssl>=1.0; python_version < \"3.7\"\n",
      "  Downloading idna-ssl-1.1.0.tar.gz (3.4 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-5.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (159 kB)\n",
      "\u001b[K     |################################| 159 kB 43.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->datasets) (20.3.0)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.2.0-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (191 kB)\n",
      "\u001b[K     |################################| 191 kB 20.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting asynctest==0.13.0; python_version < \"3.8\"\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting typing-extensions>=3.7.4; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied, skipping upgrade: urllib3>=1.25.10 in /usr/local/lib/python3.6/dist-packages (from responses<0.19->datasets) (1.26.2)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (50.3.2)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |################################| 63 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->accelerate) (0.18.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: pathtools, promise, idna-ssl\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8604 sha256=5337606c19380bff3c4c9c15212a04663c4247b5b2c65a6d4c7957d354ffd9ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/ea/90/e37d463fb3b03848bf715080595de62545266f53dd546b2497\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=23950 sha256=e78228237da068bc4c0474c5b675695c2e2415601970e9e2b6f71eb630b1e8e2\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for idna-ssl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-py3-none-any.whl size=3949 sha256=82903408ea002bbeb45e43b3eb6c6421188f09192994955340456d16e05619bd\n",
      "  Stored in directory: /root/.cache/pip/wheels/6a/f5/9c/f8331a854f7a8739cf0e74c13854e4dd7b1af11b04fe1dde13\n",
      "Successfully built pathtools promise idna-ssl\n",
      "Installing collected packages: multidict, typing-extensions, yarl, idna-ssl, async-timeout, frozenlist, aiosignal, charset-normalizer, asynctest, aiohttp, fsspec, dill, xxhash, importlib-resources, tqdm, pyarrow, multiprocess, pytz, pandas, responses, pyyaml, huggingface-hub, datasets, tokenizers, transformers, nltk, rouge-score, psutil, shortuuid, pathtools, setproctitle, docker-pycreds, promise, sentry-sdk, smmap, gitdb, GitPython, wandb, accelerate, scipy, threadpoolctl, scikit-learn\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.51.0\n",
      "    Uninstalling tqdm-4.51.0:\n",
      "      Successfully uninstalled tqdm-4.51.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.9.3\n",
      "    Uninstalling tokenizers-0.9.3:\n",
      "      Successfully uninstalled tokenizers-0.9.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 3.5.1\n",
      "    Uninstalling transformers-3.5.1:\n",
      "      Successfully uninstalled transformers-3.5.1\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "huggingface-hub 0.4.0 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n",
      "nltk 3.6.7 requires regex>=2021.8.3, but you'll have regex 2020.11.13 which is incompatible.\u001b[0m\n",
      "Successfully installed GitPython-3.1.18 accelerate-0.6.2 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-2.0.12 datasets-2.1.0 dill-0.3.4 docker-pycreds-0.4.0 frozenlist-1.2.0 fsspec-2022.1.0 gitdb-4.0.9 huggingface-hub-0.4.0 idna-ssl-1.1.0 importlib-resources-5.4.0 multidict-5.2.0 multiprocess-0.70.12.2 nltk-3.6.7 pandas-1.1.5 pathtools-0.1.2 promise-2.3 psutil-5.9.0 pyarrow-6.0.1 pytz-2022.1 pyyaml-6.0 responses-0.17.0 rouge-score-0.0.4 scikit-learn-0.24.2 scipy-1.5.4 sentry-sdk-1.5.10 setproctitle-1.2.3 shortuuid-1.0.8 smmap-5.0.0 threadpoolctl-3.1.0 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.18.0 typing-extensions-4.1.1 wandb-0.12.14 xxhash-3.0.0 yarl-1.7.2\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  git-lfs\n",
      "0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\n",
      "Need to get 2129 kB of archives.\n",
      "After this operation, 7662 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2129 kB]\n",
      "Fetched 2129 kB in 0s (12.9 MB/s)[0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package git-lfs.\n",
      "(Reading database ... 21900 files and directories currently installed.)\n",
      "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking git-lfs (2.3.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up git-lfs (2.3.4-1) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers rouge-score nltk wandb accelerate scikit-learn --upgrade\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate import meteor\n",
    "from nltk import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import *\n",
    "from torch.utils.data import *\n",
    "from accelerate import Accelerator\n",
    "from huggingface_hub import *\n",
    "from datasets import *\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "WANDB_TOKEN = '3a3cacf27e02d09574765f66bf5fb73d99dcf716'\n",
    "\n",
    "os.environ['WANDB_CONSOLE'] = 'off'\n",
    "os.environ['WANDB_API_KEY'] = WANDB_TOKEN\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    }
   ],
   "source": [
    "def generate_target_text(df, cols, is_eval=False):\n",
    "    def to_str(row):\n",
    "        res = ''\n",
    "        for col in cols:\n",
    "            res += f'{col}: {row[col]}, '\n",
    "\n",
    "        return res[:-2]\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    df = df.dropna()\n",
    "    df.loc[:, 'target_text'] = df.apply(to_str, axis=1)\n",
    "    df = df[cols + ['notes', 'target_text']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "cols_to_include = [\n",
    "    # 'event_type',\n",
    "    # 'sub_event_type',\n",
    "    'actor1',\n",
    "    'actor2',\n",
    "    # 'location',\n",
    "    # 'fatalities',\n",
    "]\n",
    "\n",
    "train = generate_target_text(\n",
    "    pd.read_csv('data/task_1_information_extraction_train.tsv', sep='\\t'),\n",
    "    cols=cols_to_include,\n",
    ")\n",
    "valid = generate_target_text(\n",
    "    pd.read_csv('data/task_1_information_extraction_valid.tsv', sep='\\t'),\n",
    "    cols=cols_to_include,\n",
    ")\n",
    "acled_df = pd.concat([train, valid])\n",
    "acled_df = acled_df.reset_index(drop=True)\n",
    "acled = Dataset.from_pandas(acled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor1</th>\n",
       "      <th>actor2</th>\n",
       "      <th>notes</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unidentified Armed Group (Pakistan)</td>\n",
       "      <td>Civilians (Pakistan)</td>\n",
       "      <td>Three people were killed while 27 others injur...</td>\n",
       "      <td>actor1: Unidentified Armed Group (Pakistan), a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Military Forces of Somalia (2012-2017)</td>\n",
       "      <td>Civilians (Somalia)</td>\n",
       "      <td>Government security forces opened fire at a pr...</td>\n",
       "      <td>actor1: Military Forces of Somalia (2012-2017)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   actor1                actor2  \\\n",
       "0     Unidentified Armed Group (Pakistan)  Civilians (Pakistan)   \n",
       "1  Military Forces of Somalia (2012-2017)   Civilians (Somalia)   \n",
       "\n",
       "                                               notes  \\\n",
       "0  Three people were killed while 27 others injur...   \n",
       "1  Government security forces opened fire at a pr...   \n",
       "\n",
       "                                         target_text  \n",
       "0  actor1: Unidentified Armed Group (Pakistan), a...  \n",
       "1  actor1: Military Forces of Somalia (2012-2017)...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acled_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69910c0c3baa48d39fa81ce6fd086f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'actor1': 'Unidentified Armed Group (Pakistan)',\n",
       " 'actor2': 'Civilians (Pakistan)',\n",
       " 'notes': 'Three people were killed while 27 others injured when a Peshawar-bound train hit a bomb planted by unidentified militants on railway tracks in Tul town in Jacobabad district in Sindh.',\n",
       " 'target_text': 'actor1: Unidentified Armed Group (Pakistan), actor2: Civilians (Pakistan)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(acled_df)\n",
    "ds.push_to_hub('vinaykudari/acled-ie-actors', token=HF_TOKEN)\n",
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/vinaykudari/t5-acled-ie-a'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = 't5-base'\n",
    "proj_name = 't5-acled-ie-a'\n",
    "ds_identifier = 'vinaykudari/acled-ie-actors'\n",
    "model_checkpoint = 't5-base'\n",
    "label_pad_token_id = -100\n",
    "repo_name = f'vinaykudari/{proj_name}'\n",
    "\n",
    "create_repo(proj_name, exist_ok=True, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(params, lr):\n",
    "    return torch.optim.Adam(params, lr)\n",
    "    \n",
    "def preprocess_function(samples, tokenizer, max_len=250, summary_len=50):\n",
    "    l = len(samples['notes'])\n",
    "    targets = samples['target_text']\n",
    "    inputs = samples['notes']\n",
    "    \n",
    "    inputs = ['summarize: ' + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "    )\n",
    "        \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            max_length=summary_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "        )\n",
    "        \n",
    "    labels['input_ids'] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
    "    ]\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "def get_cols(lis, t=True):\n",
    "    actor1_list = []\n",
    "    actor2_list = []\n",
    "\n",
    "    for text in lis:\n",
    "        actor1 = ''\n",
    "        actor2 = ''\n",
    "        \n",
    "        cols = [txt.strip() for txt in text.split(',')]\n",
    "        n = len(cols)\n",
    "        \n",
    "        for col in cols:\n",
    "            if 'actor1' in col:\n",
    "                actor1 = col.split(':')[1].strip()\n",
    "            elif 'actor2' in col:\n",
    "                actor2 = col.split(':')[1].strip()\n",
    "\n",
    "        actor1_list.append(actor1)\n",
    "        actor2_list.append(actor2)\n",
    "        \n",
    "    res = (\n",
    "        np.array(actor1_list),\n",
    "        np.array(actor2_list, dtype=object),\n",
    "    )\n",
    "        \n",
    "    return res\n",
    "        \n",
    "    \n",
    "def score(preds, labels):\n",
    "    pred_actor1, pred_actor2 = get_cols(preds)\n",
    "    actual_actor1, actual_actor2 = get_cols(labels)\n",
    "    \n",
    "    actor1_acc = (actual_actor1 == pred_actor1).mean() * 100\n",
    "    actor1_f1 = f1_score(\n",
    "        actual_actor1,\n",
    "        pred_actor1, \n",
    "        average='macro',\n",
    "    )\n",
    "    \n",
    "    actor2_acc = (actual_actor2 == pred_actor2).mean() * 100\n",
    "    actor2_f1 = f1_score(\n",
    "        actual_actor2,\n",
    "        pred_actor2, \n",
    "        average='macro',\n",
    "    )\n",
    "\n",
    "    res = { \n",
    "        'actor1_acc': actor1_acc,\n",
    "        'actor1_f1': actor1_f1,\n",
    "\n",
    "        'actor2_acc': actor2_acc,\n",
    "        'actor2_f1': actor2_f1,\n",
    "    }\n",
    "    \n",
    "    return res\n",
    "\n",
    "def evaluate(\n",
    "    model,\n",
    "    dataloader,\n",
    "    tokenizer,\n",
    "    accelerator,\n",
    "    metric,\n",
    "    progress_bar,\n",
    "):\n",
    "    step_counter = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = model.generate(\n",
    "                batch['input_ids'],\n",
    "                max_length=wandb_config.SUMMARY_LEN,\n",
    "                num_beams=wandb_config.NUM_BEAMS,\n",
    "            )\n",
    "            generated_tokens = generated_tokens.cpu().numpy()\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens,\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            \n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            scores = score(decoded_preds, decoded_labels)\n",
    "            wandb.log(scores)\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "        step_counter += 1\n",
    "        \n",
    "        if step_counter > wandb_config.MAX_EVAL_STEPS:\n",
    "            break\n",
    "                \n",
    "\n",
    "def train_one_epoch(\n",
    "    model, \n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    accelerator,\n",
    "    lr_scheduler,\n",
    "    progress_bar,\n",
    "    n_epoch,\n",
    "    tot_steps,\n",
    "):\n",
    "    step_count = 0\n",
    "    \n",
    "    for step, batch in enumerate(dataloader):\n",
    "        print(f'.', end='')\n",
    "        tot_steps += 1\n",
    "        \n",
    "        preds = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            labels=batch['labels'],\n",
    "        )\n",
    "        loss = preds.loss\n",
    "        loss /= wandb_config.GRAD_ACCUM_STEPS\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        wandb.log(\n",
    "            {\n",
    "                'loss': loss,\n",
    "                'perplixity': torch.exp(loss),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        if step % wandb_config.GRAD_ACCUM_STEPS == 0 or step == len(dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            step_count += 1\n",
    "\n",
    "        if step_count >= wandb_config.MAX_TRAIN_STEPS:\n",
    "            break\n",
    "            \n",
    "    return tot_steps\n",
    "\n",
    "def save_push_to_hub(accelerater, commit_msg):\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(proj_name, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        model.push_to_hub(repo_name)\n",
    "        tokenizer.push_to_hub(repo_name)\n",
    "\n",
    "def run(optim, model, dataset, config, tokenizer):         \n",
    "    torch.manual_seed(wandb_config.SEED)\n",
    "    np.random.seed(wandb_config.SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    if accelerator.is_main_process:\n",
    "        repo = Repository(repo_name, clone_from=repo_name)\n",
    "        \n",
    "    accelerator.wait_for_everyone()\n",
    "    \n",
    "    remove_columns = dataset['train'].column_names\n",
    "    \n",
    "    # when trainer model is different from the tokenizer model\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        tokenized_ds = dataset.map(\n",
    "            preprocess_function,\n",
    "            num_proc=8,\n",
    "            batched=True,\n",
    "            remove_columns=remove_columns,\n",
    "            fn_kwargs={\n",
    "                'tokenizer': tokenizer,\n",
    "                'max_len': wandb_config.MAX_TEXT_LEN,\n",
    "                'summary_len': wandb_config.SUMMARY_LEN\n",
    "            },\n",
    "            load_from_cache_file=True,\n",
    "        )\n",
    "    \n",
    "    train_dataset = tokenized_ds['train']\n",
    "    eval_dataset = tokenized_ds['test']\n",
    "    \n",
    "    # form a batch by using a list of dataset elements\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer, \n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "    )\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        collate_fn=data_collator,\n",
    "        batch_size=wandb_config.TRAIN_BATCH_SIZE\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        collate_fn=data_collator,\n",
    "        batch_size=wandb_config.EVAL_BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    model_params = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': wandb_config.WEIGHT_DECAY,\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'weight_decay': 0.0,\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    optimizer = optim(model_params, wandb_config.LEARNING_RATE)\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "    \n",
    "    # log metrics with wandb\n",
    "    wandb.watch(model, log='all')\n",
    "    \n",
    "    n_update_steps = math.ceil(len(train_dataloader) / wandb_config.GRAD_ACCUM_STEPS)\n",
    "    if wandb_config.MAX_TRAIN_STEPS is None:\n",
    "        wandb_config.MAX_TRAIN_STEPS = wandb_config.TRAIN_EPOCHS * n_update_steps\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "        name='linear',\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=wandb_config.LR_WARM_UP_STEPS,\n",
    "        num_training_steps=wandb_config.MAX_TRAIN_STEPS,\n",
    "    )\n",
    "    \n",
    "    print('Training')\n",
    "    metric = load_metric('rouge')\n",
    "    train_p_bar = tqdm(\n",
    "        range(wandb_config.MAX_TRAIN_STEPS),\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    eval_p_bar = tqdm(\n",
    "        range(wandb_config.MAX_EVAL_STEPS),\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    tot_steps = 0\n",
    "    \n",
    "    for n_epoch in range(wandb_config.TRAIN_EPOCHS):\n",
    "        # set model to traning mode\n",
    "        model.train()\n",
    "        tot_steps = train_one_epoch(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            optimizer,\n",
    "            accelerator,\n",
    "            lr_scheduler,\n",
    "            train_p_bar,\n",
    "            n_epoch, \n",
    "            tot_steps,\n",
    "        )\n",
    "        print('\\nValidation')\n",
    "#         set model to evaluation mode\n",
    "        model.eval()\n",
    "        evaluate(\n",
    "            model,\n",
    "            eval_dataloader,\n",
    "            tokenizer,\n",
    "            accelerator,\n",
    "            metric,\n",
    "            eval_p_bar,\n",
    "        )\n",
    "#         save_push_to_hub(accelerator, f'Traning {n_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration vinaykudari--acled-ie-actors-7e1252d290873b4b\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/vinaykudari--acled-ie-actors-7e1252d290873b4b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvkudari\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/tasks/wandb/run-20220508_170115-1fbkf4zl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/vkudari/t5-acled-ie-a/runs/1fbkf4zl\" target=\"_blank\">quiet-salad-1</a></strong> to <a href=\"https://wandb.ai/vkudari/t5-acled-ie-a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/vinaykudari/t5-acled-ie-a into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ca172d8d5b4ec29df5b44138d3a41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff622698d6f435588fe3bbc6898a720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b9d31dca5441968ace3e09dee97cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2e47b843e34348bbfc92d4271c1677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e5a543610d4b0b89ab365dd1354efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0dc6f710c14a32a4fe735b57af3095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876297039e23487a8d7e71a640254ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc85dd55fda44dc5882f252fa4770e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "923e1bfe007d4365a4924e523ac19d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a87405ffa76458b971912ac8fd69179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ec1106c65d4ce8b2813c99cdcbaee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addc4d5d0efc498e984867b6bc2a25e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881d925ac93444ac90eeaaed398ac88d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8d191e6d974a6e938705ebb5ab6500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b36ced1b0b04b6ea6e93ba210d3aa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06b5ccac64f4660bc3d967c73f2d9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfd614c973242f1bef45b74876b47f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5da27c0f24481e8e30afbf24a0ad55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Validation\n",
      "..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>actor1_acc</td><td>▃▆▃▂▄▃▅▅▃▃▂▃▅▅▃█▂▅▃▄▁▄▂▄▃▆▁▄▂█▃▂▁▄▅▂▅▆▅▄</td></tr><tr><td>actor1_f1</td><td>▃▆▄▂▃▃▄▄▃▃▂▃▅▄▂▅▂▄▃▄▁▄▂▄▃▆▁▃▂█▃▂▂▃▄▂▅▅▅▄</td></tr><tr><td>actor2_acc</td><td>▃▅▂▆▆▅▃▃▇▂▃▅▆▆▅▇▅▇▅▅▂▃▁▃▁█▃▃▃█▅▅▂▂▆▅▃▇▅▅</td></tr><tr><td>actor2_f1</td><td>▄▄▃▅▅▄▃▃█▁▄▅▇▅▅▄▅▆▄▄▃▃▁▄▁█▂▃▃█▄▄▃▃▆▅▄▆▅▃</td></tr><tr><td>loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▂▂▁▁▁▂▁▁▂▁▁▁▂▁▁▂▁▂▁▂</td></tr><tr><td>perplixity</td><td>█▄▃▃▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>actor1_acc</td><td>20.0</td></tr><tr><td>actor1_f1</td><td>0.12963</td></tr><tr><td>actor2_acc</td><td>15.0</td></tr><tr><td>actor2_f1</td><td>0.04021</td></tr><tr><td>loss</td><td>0.13062</td></tr><tr><td>perplixity</td><td>1.13953</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">quiet-salad-1</strong>: <a href=\"https://wandb.ai/vkudari/t5-acled-ie-a/runs/1fbkf4zl\" target=\"_blank\">https://wandb.ai/vkudari/t5-acled-ie-a/runs/1fbkf4zl</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220508_170115-1fbkf4zl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accelerator = Accelerator(fp16=True)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, config=config)\n",
    "block_size = tokenizer.model_max_length\n",
    "\n",
    "acled = load_dataset(ds_identifier, split='train')\n",
    "acled = acled.train_test_split(0.1)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "TRAIN_BATCH_SIZE = 20\n",
    "EVAL_BATCH_SIZE = 20\n",
    "TRAIN_EPOCHS = 2\n",
    "SEED = 42             \n",
    "MAX_TEXT_LEN = 250\n",
    "SUMMARY_LEN = 50 \n",
    "WEIGHT_DECAY = 0.0\n",
    "GRAD_ACCUM_STEPS = 5\n",
    "NUM_BEAMS = 2\n",
    "LR_WARM_UP_STEPS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "MAX_TRAIN_STEPS = 150\n",
    "MAX_EVAL_STEPS = 100\n",
    "\n",
    "n_epochs = TRAIN_EPOCHS\n",
    "config = {'n_epochs': n_epochs}\n",
    "w = wandb.init(project=proj_name, config=config)\n",
    "wandb_config = wandb.config \n",
    "\n",
    "\n",
    "wandb_config.TRAIN_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "wandb_config.EVAL_BATCH_SIZE = EVAL_BATCH_SIZE\n",
    "wandb_config.TRAIN_EPOCHS = TRAIN_EPOCHS\n",
    "wandb_config.SEED = SEED          \n",
    "wandb_config.MAX_TEXT_LEN = MAX_TEXT_LEN\n",
    "wandb_config.SUMMARY_LEN = SUMMARY_LEN\n",
    "wandb_config.WEIGHT_DECAY = WEIGHT_DECAY\n",
    "wandb_config.GRAD_ACCUM_STEPS = GRAD_ACCUM_STEPS\n",
    "wandb_config.NUM_BEAMS = NUM_BEAMS\n",
    "wandb_config.LR_WARM_UP_STEPS = LR_WARM_UP_STEPS\n",
    "wandb_config.LEARNING_RATE = LEARNING_RATE\n",
    "wandb_config.MAX_TRAIN_STEPS = MAX_TRAIN_STEPS\n",
    "wandb_config.MAX_EVAL_STEPS = MAX_EVAL_STEPS\n",
    "\n",
    "run(\n",
    "    optim=optimizer, \n",
    "    model=model,\n",
    "    dataset=acled,\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "w.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cdbbc963c7446cb28c30dee3c9dc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 3.34k/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/vinaykudari/t5-acled-ie\n",
      "   ca5966f..bb512ba  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/vinaykudari/t5-acled-ie/commit/bb512ba2c45897b6eebefd14a54f2ec88fe37a02'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_columns = acled['train'].column_names\n",
    "# acled_ds = acled.map(\n",
    "#     preprocess_function,\n",
    "#     batched=True,\n",
    "#     num_proc=4,\n",
    "#     remove_columns=remove_columns,\n",
    "#     fn_kwargs={\n",
    "#         'tokenizer': tokenizer,\n",
    "#         'max_len': 250,\n",
    "#         'summary_len': 50,\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = 'vinaykudari/t5-acled-ie'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_eval)\n",
    "# eval_tokenizer = AutoTokenizer.from_pretrained(model_eval)\n",
    "# eval_dataset = acled_ds['test']\n",
    "# eval_data_collator = DataCollatorForSeq2Seq(\n",
    "#     eval_tokenizer, \n",
    "#     model=eval_model,\n",
    "#     padding='longest',\n",
    "#     label_pad_token_id=label_pad_token_id,\n",
    "# )\n",
    "\n",
    "\n",
    "# eval_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 20\n",
    "# acled_ds['test'][n]['notes']\n",
    "\n",
    "# test = torch.tensor([acled_ds['test'][n]['input_ids']])\n",
    "# res = eval_model.generate(test, max_length=50)\n",
    "# eval_tokenizer.decode(res[0], skip_special_tokens=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocess_function(samples, tokenizer, max_len=250, summary_len=50):\n",
    "    l = len(samples['notes'])\n",
    "    inputs = samples['notes']\n",
    "    \n",
    "    inputs = ['summarize: ' + inp for inp in inputs]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5867309e914bc88247d598a9456fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea0b5cfc9974058a1e3bc188efb391b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665a9fa998384d0b8a6e6a7d3dc72672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d264d2fba24913b17152d2d4c565ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_df = pd.read_csv('data/task_1_information_extraction_valid.tsv', sep='\\t')\n",
    "valid_df.columns = [col.lower() for col in valid_df.columns]\n",
    "valid_df = valid_df.dropna()\n",
    "\n",
    "model.eval()\n",
    "eval_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, \n",
    "    model=model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    ")\n",
    "eval_ds = Dataset.from_pandas(valid_df)\n",
    "remove_columns = eval_ds.column_names\n",
    "eval_tds = eval_ds.map(\n",
    "    eval_preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=remove_columns,\n",
    "    num_proc=4,\n",
    "    fn_kwargs={\n",
    "        'tokenizer': tokenizer,\n",
    "        'max_len': 250,\n",
    "        'summary_len': 50,\n",
    "    },\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_tds, \n",
    "    collate_fn=eval_data_collator,\n",
    "    batch_size=EVAL_BATCH_SIZE,\n",
    ")\n",
    "\n",
    "# ed_lis = list(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................'NoneType' object has no attribute '_log'\n",
      "................................'NoneType' object has no attribute '_log'\n",
      ".............................'NoneType' object has no attribute '_log'\n",
      "...........................'NoneType' object has no attribute '_log'\n",
      "..........................'NoneType' object has no attribute '_log'\n",
      "..........................'NoneType' object has no attribute '_log'\n",
      ".............................'NoneType' object has no attribute '_log'\n",
      ".........................'NoneType' object has no attribute '_log'\n",
      "...........................'NoneType' object has no attribute '_log'\n",
      ".....................'NoneType' object has no attribute '_log'\n",
      "...............................'NoneType' object has no attribute '_log'\n",
      "..............................'NoneType' object has no attribute '_log'\n",
      "................................'NoneType' object has no attribute '_log'\n",
      "...............................'NoneType' object has no attribute '_log'\n",
      ".............................'NoneType' object has no attribute '_log'\n",
      "............................'NoneType' object has no attribute '_log'\n",
      "............................'NoneType' object has no attribute '_log'\n",
      "............................'NoneType' object has no attribute '_log'\n",
      "............"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "_ = model.to(device)\n",
    "\n",
    "actor1_list = []\n",
    "actor2_list = []\n",
    "\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    print('.', end='')\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            generated_tokens = model.generate(\n",
    "                torch.tensor(batch['input_ids'], device=device),\n",
    "                max_length=SUMMARY_LEN,\n",
    "            )\n",
    "\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens,\n",
    "                skip_special_tokens=True,\n",
    "                max_length=MAX_TEXT_LEN,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "            )\n",
    "\n",
    "            pred_actor1, pred_actor2 = get_cols(decoded_preds)\n",
    "            actor1_list += pred_actor1.tolist()\n",
    "            actor2_list += pred_actor2.tolist()\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = valid_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df['pred_location'] = location_list\n",
    "valid_df['pred_fatalities'] = fatality_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>notes</th>\n",
       "      <th>event_date</th>\n",
       "      <th>source</th>\n",
       "      <th>fatalities</th>\n",
       "      <th>event_type</th>\n",
       "      <th>sub_event_type</th>\n",
       "      <th>actor1</th>\n",
       "      <th>inter1</th>\n",
       "      <th>actor2</th>\n",
       "      <th>inter2</th>\n",
       "      <th>interaction</th>\n",
       "      <th>location</th>\n",
       "      <th>pred_location</th>\n",
       "      <th>pred_fatalities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al Shabaab members attack and kill two men tra...</td>\n",
       "      <td>25-February-2013</td>\n",
       "      <td>Undisclosed Source</td>\n",
       "      <td>2</td>\n",
       "      <td>Violence against civilians</td>\n",
       "      <td>Attack</td>\n",
       "      <td>Al Shabaab</td>\n",
       "      <td>2</td>\n",
       "      <td>Civilians (Somalia)</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>Baadhaade</td>\n",
       "      <td>Badhaade</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arrests: Al Shabaab militias arrested a group ...</td>\n",
       "      <td>16-December-2012</td>\n",
       "      <td>Undisclosed Source</td>\n",
       "      <td>0</td>\n",
       "      <td>Strategic developments</td>\n",
       "      <td>Arrests</td>\n",
       "      <td>Al Shabaab</td>\n",
       "      <td>2</td>\n",
       "      <td>Civilians (Somalia)</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>Baadhaade</td>\n",
       "      <td>Mogadishu - Al Shabaab</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMISOM/Somali forces clash with al Shabaab in ...</td>\n",
       "      <td>26-September-2013</td>\n",
       "      <td>Shabelle Media Network</td>\n",
       "      <td>0</td>\n",
       "      <td>Battles</td>\n",
       "      <td>Armed clash</td>\n",
       "      <td>Military Forces of Somalia (2012-2017)</td>\n",
       "      <td>1</td>\n",
       "      <td>Al Shabaab</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>Baadhaade</td>\n",
       "      <td>Kulbiyow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AS forces took back Diif village from Militia ...</td>\n",
       "      <td>12-March-2011</td>\n",
       "      <td>Undisclosed Source</td>\n",
       "      <td>0</td>\n",
       "      <td>Battles</td>\n",
       "      <td>Armed clash</td>\n",
       "      <td>Al Shabaab</td>\n",
       "      <td>2</td>\n",
       "      <td>Militia (Ahmed Madoobe)</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>Baadhaade</td>\n",
       "      <td>Diif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unidentified armed men picked a civilian man f...</td>\n",
       "      <td>06-November-2013</td>\n",
       "      <td>Undisclosed Source</td>\n",
       "      <td>1</td>\n",
       "      <td>Violence against civilians</td>\n",
       "      <td>Attack</td>\n",
       "      <td>Al Shabaab</td>\n",
       "      <td>2</td>\n",
       "      <td>Civilians (Somalia)</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>Mogadishu - Karan</td>\n",
       "      <td>Mogadishu - Kaaraan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               notes         event_date  \\\n",
       "0  Al Shabaab members attack and kill two men tra...   25-February-2013   \n",
       "1  Arrests: Al Shabaab militias arrested a group ...   16-December-2012   \n",
       "2  AMISOM/Somali forces clash with al Shabaab in ...  26-September-2013   \n",
       "3  AS forces took back Diif village from Militia ...      12-March-2011   \n",
       "4  Unidentified armed men picked a civilian man f...   06-November-2013   \n",
       "\n",
       "                   source  fatalities                  event_type  \\\n",
       "0      Undisclosed Source           2  Violence against civilians   \n",
       "1      Undisclosed Source           0      Strategic developments   \n",
       "2  Shabelle Media Network           0                     Battles   \n",
       "3      Undisclosed Source           0                     Battles   \n",
       "4      Undisclosed Source           1  Violence against civilians   \n",
       "\n",
       "  sub_event_type                                  actor1  inter1  \\\n",
       "0         Attack                              Al Shabaab       2   \n",
       "1        Arrests                              Al Shabaab       2   \n",
       "2    Armed clash  Military Forces of Somalia (2012-2017)       1   \n",
       "3    Armed clash                              Al Shabaab       2   \n",
       "4         Attack                              Al Shabaab       2   \n",
       "\n",
       "                    actor2  inter2  interaction           location  \\\n",
       "0      Civilians (Somalia)       7           27          Baadhaade   \n",
       "1      Civilians (Somalia)       7           27          Baadhaade   \n",
       "2               Al Shabaab       2           12          Baadhaade   \n",
       "3  Militia (Ahmed Madoobe)       3           23          Baadhaade   \n",
       "4      Civilians (Somalia)       7           27  Mogadishu - Karan   \n",
       "\n",
       "            pred_location pred_fatalities  \n",
       "0                Badhaade               2  \n",
       "1  Mogadishu - Al Shabaab               0  \n",
       "2                Kulbiyow               0  \n",
       "3                    Diif               0  \n",
       "4     Mogadishu - Kaaraan               1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df.to_csv('task1_valid_actor.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
