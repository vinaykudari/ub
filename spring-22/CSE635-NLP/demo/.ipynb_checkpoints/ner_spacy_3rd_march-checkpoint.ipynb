{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffea903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a spacy model and chekc if it has ner\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d26f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India GPE\n",
      "one CARDINAL\n",
      "Indian NORP\n",
      "USD 84 billion MONEY\n",
      "2021 DATE\n",
      "USD 24 billion MONEY\n",
      "2017 DATE\n",
      "India GPE\n",
      "Philip PERSON\n",
      "12% PERCENT\n",
      "2017 DATE\n",
      "22-25% PERCENT\n",
      "2021 DATE\n",
      "India GPE\n",
      "One CARDINAL\n",
      "Amazon ORG\n",
      "Amazon ORG\n",
      "Amazon Music Limited ORG\n",
      "2007 DATE\n",
      "Flipkart PERSON\n",
      "Indian NORP\n",
      "Amazon ORG\n",
      "US GPE\n",
      "Amazon ORG\n",
      "daily DATE\n",
      "2010 DATE\n",
      "Snapdeal GPE\n",
      "2011 DATE\n",
      "more than 3 CARDINAL\n",
      "India GPE\n",
      "over 30 million CARDINAL\n",
      "over 125,000 CARDINAL\n",
      "Indian NORP\n",
      "recent years DATE\n",
      "Freecharge PERSON\n",
      "Unicommerce ORG\n",
      "Indian NORP\n",
      "July 2011 DATE\n",
      "Gurugram ORG\n",
      "INR ORG\n",
      "1.1 billion CARDINAL\n",
      "Nexus Venture Partners ORG\n",
      "Tiger Global PERSON\n",
      "Helion Ventures ORG\n",
      "more than 5 CARDINAL\n",
      "nine CARDINAL\n",
      "Paytm Mall PERSON\n",
      "Paytm ORG\n",
      "Paytm Mall PERSON\n",
      "third ORDINAL\n",
      "Paytm PERSON\n",
      "Reliance Retail PERSON\n",
      "Reliance Jio’s PERSON\n",
      "Indian NORP\n",
      "Reliance ORG\n",
      "Reliance ORG\n",
      "India GPE\n",
      "Big Basket ORG\n",
      "two CARDINAL\n",
      "One CARDINAL\n",
      "Grofers ORG\n",
      "2013 DATE\n",
      "the last 5 years DATE\n",
      "atta ORG\n",
      "daily DATE\n",
      "India GPE\n",
      "Indian NORP\n",
      "Digital Mall PERSON\n",
      "Asia LOC\n",
      "2020 DATE\n",
      "Digital Mall ORG\n",
      "Asia LOC\n",
      "Yokeasia GPE\n",
      "zero CARDINAL\n",
      "monthly DATE\n",
      "one CARDINAL\n",
      "DMA ORG\n"
     ]
    }
   ],
   "source": [
    "# Performing NER on E-commerce article\n",
    "\n",
    "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
    "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
    "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
    "Amazon – One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
    "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
    "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
    "ShopClues – Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
    "Paytm Mall – To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace – Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app – Paytm. \n",
    "Reliance Retail – Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
    "Big Basket – India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services – express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
    "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
    "Digital Mall of Asia – Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
    "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting. \"\"\"\n",
    "\n",
    "doc=nlp(article_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d895e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5516a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d786c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "from spacy.training.example import Example\n",
    "\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed8c7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0abc54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b231eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a old table\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a screwdriver from our neighbour\" with entities \"[(12, 22, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a camera\" with entities \"[(12, 18, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a fancy dress\" with entities \"[(18, 23, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered a book from Amazon\" with entities \"[(24, 32, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I repaired my computer\" with entities \"[(15, 23, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a tent for our trip\" with entities \"[(12, 16, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a new Washer\" with entities \"[(16, 22, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered from Swiggy\" with entities \"[(24, 29, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I got my truck fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I reached Chennai yesterday.\" with entities \"[(19, 28, 'GPE')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"I got my clock fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.003482781235270238}\n",
      "Losses {'ner': 5.326986785646277}\n",
      "Losses {'ner': 8.073704454878076}\n",
      "Losses {'ner': 10.857094899321531}\n",
      "Losses {'ner': 14.849511488623797}\n",
      "Losses {'ner': 3.1797061421043127}\n",
      "Losses {'ner': 8.66447136963912}\n",
      "Losses {'ner': 12.560020803727708}\n",
      "Losses {'ner': 13.063895810131028}\n",
      "Losses {'ner': 13.06441401135197}\n",
      "Losses {'ner': 2.204561967641273}\n",
      "Losses {'ner': 6.038974447264256}\n",
      "Losses {'ner': 7.589321294247409}\n",
      "Losses {'ner': 8.363619992251667}\n",
      "Losses {'ner': 8.48985813025686}\n",
      "Losses {'ner': 1.6493239898018146}\n",
      "Losses {'ner': 4.849847211888606}\n",
      "Losses {'ner': 5.687331334705614}\n",
      "Losses {'ner': 7.532543496238868}\n",
      "Losses {'ner': 7.538031654613262}\n",
      "Losses {'ner': 1.8092520002132915}\n",
      "Losses {'ner': 1.9250350921286739}\n",
      "Losses {'ner': 3.0716665784633506}\n",
      "Losses {'ner': 6.419512973285904}\n",
      "Losses {'ner': 6.430738308931311}\n",
      "Losses {'ner': 0.24068305874268087}\n",
      "Losses {'ner': 5.081464329681678}\n",
      "Losses {'ner': 6.226012921265221}\n",
      "Losses {'ner': 7.909120580301915}\n",
      "Losses {'ner': 7.909121131982876}\n",
      "Losses {'ner': 0.594026938318045}\n",
      "Losses {'ner': 2.6516123493398998}\n",
      "Losses {'ner': 2.66250805068664}\n",
      "Losses {'ner': 5.11396832190119}\n",
      "Losses {'ner': 6.952377621734014}\n",
      "Losses {'ner': 0.6651239463685623}\n",
      "Losses {'ner': 1.762904419601312}\n",
      "Losses {'ner': 1.7630407440273663}\n",
      "Losses {'ner': 1.767949926953245}\n",
      "Losses {'ner': 1.7690066963333955}\n",
      "Losses {'ner': 0.23607839479956283}\n",
      "Losses {'ner': 1.95276720831421}\n",
      "Losses {'ner': 1.9554804354132436}\n",
      "Losses {'ner': 5.1812613548406175}\n",
      "Losses {'ner': 5.1812623855996405}\n",
      "Losses {'ner': 0.6990856878165229}\n",
      "Losses {'ner': 0.6991367260821932}\n",
      "Losses {'ner': 0.6992874950777108}\n",
      "Losses {'ner': 0.6994988417510023}\n",
      "Losses {'ner': 2.2700438992333605}\n",
      "Losses {'ner': 7.667266137524012e-08}\n",
      "Losses {'ner': 0.034905842446328106}\n",
      "Losses {'ner': 3.2019201810707574}\n",
      "Losses {'ner': 3.2025061842332834}\n",
      "Losses {'ner': 3.20479981818327}\n",
      "Losses {'ner': 0.41616987436531766}\n",
      "Losses {'ner': 0.41705138676944153}\n",
      "Losses {'ner': 0.4305076025141361}\n",
      "Losses {'ner': 3.495805410468061}\n",
      "Losses {'ner': 3.495805488542884}\n",
      "Losses {'ner': 1.7584705920435886}\n",
      "Losses {'ner': 3.745755117000865}\n",
      "Losses {'ner': 4.30409201014913}\n",
      "Losses {'ner': 4.3041442330369595}\n",
      "Losses {'ner': 4.304174944288188}\n",
      "Losses {'ner': 0.000226289542850446}\n",
      "Losses {'ner': 0.0009654493868846281}\n",
      "Losses {'ner': 0.0014907343481620323}\n",
      "Losses {'ner': 1.6585514090782911}\n",
      "Losses {'ner': 2.8655466876477895}\n",
      "Losses {'ner': 8.25559867493481e-07}\n",
      "Losses {'ner': 6.165198639684477e-06}\n",
      "Losses {'ner': 0.008070991836222377}\n",
      "Losses {'ner': 0.016613931206541244}\n",
      "Losses {'ner': 0.016770248738214383}\n",
      "Losses {'ner': 0.008922685784892487}\n",
      "Losses {'ner': 0.01000812421050383}\n",
      "Losses {'ner': 0.010147281127543803}\n",
      "Losses {'ner': 0.01016666687161808}\n",
      "Losses {'ner': 0.011474197083981593}\n",
      "Losses {'ner': 0.07535095571530101}\n",
      "Losses {'ner': 0.07616242939806762}\n",
      "Losses {'ner': 0.07616249696887512}\n",
      "Losses {'ner': 0.07621456922943338}\n",
      "Losses {'ner': 0.07621489556797709}\n",
      "Losses {'ner': 0.7969964749896261}\n",
      "Losses {'ner': 0.799950092449245}\n",
      "Losses {'ner': 0.7999505509310736}\n",
      "Losses {'ner': 0.8004764669947032}\n",
      "Losses {'ner': 2.735390557590979}\n",
      "Losses {'ner': 2.7355607048741527e-07}\n",
      "Losses {'ner': 0.0037194515371055366}\n",
      "Losses {'ner': 0.004460218912426188}\n",
      "Losses {'ner': 0.005032396260126813}\n",
      "Losses {'ner': 0.005032397530913116}\n",
      "Losses {'ner': 7.931007953458046e-08}\n",
      "Losses {'ner': 2.8776968970326226e-07}\n",
      "Losses {'ner': 0.004752392717481166}\n",
      "Losses {'ner': 0.004764304131314521}\n",
      "Losses {'ner': 0.004765351056700627}\n",
      "Losses {'ner': 1.3456764042115042e-06}\n",
      "Losses {'ner': 1.3516954458178758e-06}\n",
      "Losses {'ner': 5.983172565146531e-06}\n",
      "Losses {'ner': 0.05206185774825568}\n",
      "Losses {'ner': 0.05206829403864735}\n",
      "Losses {'ner': 6.0568512282197714e-05}\n",
      "Losses {'ner': 0.1571929816483386}\n",
      "Losses {'ner': 0.1571929848365588}\n",
      "Losses {'ner': 0.15719386115048173}\n",
      "Losses {'ner': 0.15719415184025254}\n",
      "Losses {'ner': 0.006317729434942415}\n",
      "Losses {'ner': 0.006317764456635161}\n",
      "Losses {'ner': 0.006323847496029659}\n",
      "Losses {'ner': 0.17473874093410455}\n",
      "Losses {'ner': 0.17473874768191097}\n",
      "Losses {'ner': 4.086694670321275e-07}\n",
      "Losses {'ner': 8.105599592747431e-07}\n",
      "Losses {'ner': 9.003863159872385e-07}\n",
      "Losses {'ner': 2.700980887875372e-06}\n",
      "Losses {'ner': 3.8314886798373876e-06}\n",
      "Losses {'ner': 0.00013057792615625482}\n",
      "Losses {'ner': 1.8253534577658064}\n",
      "Losses {'ner': 1.8253535269985965}\n",
      "Losses {'ner': 1.8254038540649369}\n",
      "Losses {'ner': 1.8254058298791986}\n",
      "Losses {'ner': 0.004622884204635822}\n",
      "Losses {'ner': 0.00462291793224245}\n",
      "Losses {'ner': 0.005387763997509019}\n",
      "Losses {'ner': 0.005395037737720314}\n",
      "Losses {'ner': 0.005395268315022838}\n",
      "Losses {'ner': 2.3493920117078415e-08}\n",
      "Losses {'ner': 2.9767194599929406e-07}\n",
      "Losses {'ner': 8.103402852506788e-07}\n",
      "Losses {'ner': 1.9580727200988968}\n",
      "Losses {'ner': 1.958072747323897}\n",
      "Losses {'ner': 1.4650471214388352e-08}\n",
      "Losses {'ner': 2.0214521506884166e-06}\n",
      "Losses {'ner': 7.494004465768456e-06}\n",
      "Losses {'ner': 7.500909586030515e-06}\n",
      "Losses {'ner': 7.639466487844928e-06}\n",
      "Losses {'ner': 0.046308976882326824}\n",
      "Losses {'ner': 0.0747785759741415}\n",
      "Losses {'ner': 0.07478209417734712}\n",
      "Losses {'ner': 0.0747821613380408}\n",
      "Losses {'ner': 0.07478425100422394}\n",
      "Losses {'ner': 1.5880743009023519e-07}\n",
      "Losses {'ner': 1.618891325945877e-07}\n",
      "Losses {'ner': 1.4669026591966894}\n",
      "Losses {'ner': 1.4669584092431975}\n",
      "Losses {'ner': 1.4669584954578032}\n",
      "Losses {'ner': 0.0019665892225032688}\n",
      "Losses {'ner': 0.0019675734222292587}\n",
      "Losses {'ner': 0.00439056287540842}\n",
      "Losses {'ner': 0.004390837003443197}\n",
      "Losses {'ner': 0.0043908382429345885}\n",
      "Losses {'ner': 0.0002287637175249775}\n",
      "Losses {'ner': 0.00022877154516336422}\n",
      "Losses {'ner': 0.00022877205389087687}\n",
      "Losses {'ner': 0.00022902955312055658}\n",
      "Losses {'ner': 0.00022903191378376825}\n",
      "Losses {'ner': 2.5710310490349417e-08}\n",
      "Losses {'ner': 1.2111003048171225e-06}\n",
      "Losses {'ner': 0.0011009123892411457}\n",
      "Losses {'ner': 0.001100912389968151}\n",
      "Losses {'ner': 0.0011009138932998625}\n",
      "Losses {'ner': 7.296265921381057e-11}\n",
      "Losses {'ner': 3.4001093228950115e-05}\n",
      "Losses {'ner': 3.40011511110003e-05}\n",
      "Losses {'ner': 3.402342628884973e-05}\n",
      "Losses {'ner': 9.804062087152881e-05}\n",
      "Losses {'ner': 2.853119389167674e-05}\n",
      "Losses {'ner': 3.05588775772409e-05}\n",
      "Losses {'ner': 3.0705275252768025e-05}\n",
      "Losses {'ner': 3.183161083314074e-05}\n",
      "Losses {'ner': 3.183179124231718e-05}\n",
      "Losses {'ner': 2.1090680655857906e-06}\n",
      "Losses {'ner': 9.984764963681555e-06}\n",
      "Losses {'ner': 1.0137503853278076e-05}\n",
      "Losses {'ner': 1.2673539047885435e-05}\n",
      "Losses {'ner': 1.908797782268302e-05}\n",
      "Losses {'ner': 1.1630274529315758e-07}\n",
      "Losses {'ner': 1.1630807784611134e-07}\n",
      "Losses {'ner': 1.1786317738565916e-07}\n",
      "Losses {'ner': 1.1887493055792313e-07}\n",
      "Losses {'ner': 1.567492983178255e-07}\n",
      "Losses {'ner': 2.7143728158359626e-09}\n",
      "Losses {'ner': 0.00032024921998995436}\n",
      "Losses {'ner': 0.00032026659602161795}\n",
      "Losses {'ner': 0.0003210849783266186}\n",
      "Losses {'ner': 0.000321085097764382}\n",
      "Losses {'ner': 0.00022458950415967695}\n",
      "Losses {'ner': 0.00022458950417404504}\n",
      "Losses {'ner': 0.00022465484371154442}\n",
      "Losses {'ner': 1.4684649424036553}\n",
      "Losses {'ner': 1.468464942404658}\n",
      "Losses {'ner': 6.4326557826465785e-06}\n",
      "Losses {'ner': 6.438388138362161e-06}\n",
      "Losses {'ner': 0.00017761132234691117}\n",
      "Losses {'ner': 0.00017777676827386164}\n",
      "Losses {'ner': 0.0005460191558948403}\n",
      "Losses {'ner': 1.7547034716786047e-08}\n",
      "Losses {'ner': 2.720662358259997e-07}\n",
      "Losses {'ner': 0.000128980258314554}\n",
      "Losses {'ner': 0.0001289802846394911}\n",
      "Losses {'ner': 0.04389704940962535}\n",
      "Losses {'ner': 1.3961514452765514e-09}\n",
      "Losses {'ner': 1.1267243358461546e-08}\n",
      "Losses {'ner': 0.00011472365670217817}\n",
      "Losses {'ner': 0.00011472367047815777}\n",
      "Losses {'ner': 0.00011472576323274115}\n",
      "Losses {'ner': 2.896464561168639e-07}\n",
      "Losses {'ner': 0.0009614153605186666}\n",
      "Losses {'ner': 0.0009614155146108315}\n",
      "Losses {'ner': 0.0009614155218356136}\n",
      "Losses {'ner': 0.0009614157943798446}\n",
      "Losses {'ner': 1.4731508962948602e-09}\n",
      "Losses {'ner': 1.7121911130390748e-06}\n",
      "Losses {'ner': 8.94581140476507e-05}\n",
      "Losses {'ner': 0.00039383998516640526}\n",
      "Losses {'ner': 0.00039384297065641434}\n",
      "Losses {'ner': 6.774079046530081e-05}\n",
      "Losses {'ner': 6.774081446745803e-05}\n",
      "Losses {'ner': 6.774089211417886e-05}\n",
      "Losses {'ner': 6.774689619995736e-05}\n",
      "Losses {'ner': 6.775968362060858e-05}\n",
      "Losses {'ner': 1.3096444786866234e-13}\n",
      "Losses {'ner': 1.708873263971205e-07}\n",
      "Losses {'ner': 4.8673541429124644e-05}\n",
      "Losses {'ner': 4.8679433498589936e-05}\n",
      "Losses {'ner': 4.867987424445053e-05}\n",
      "Losses {'ner': 2.3637528946015045e-08}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 0.2870737113815431}\n",
      "Losses {'ner': 0.2870906907239863}\n",
      "Losses {'ner': 0.2871495810172871}\n",
      "Losses {'ner': 0.28714958101773946}\n",
      "Losses {'ner': 0.15508827569021882}\n",
      "Losses {'ner': 0.1550882756935798}\n",
      "Losses {'ner': 0.1550882761654434}\n",
      "Losses {'ner': 0.15508827716334278}\n",
      "Losses {'ner': 0.15508827716448412}\n",
      "Losses {'ner': 5.758237135586316e-10}\n",
      "Losses {'ner': 6.157951571819666e-10}\n",
      "Losses {'ner': 1.0337947768408971e-09}\n",
      "Losses {'ner': 1.3507955351180995e-06}\n",
      "Losses {'ner': 1.4551339046456585e-06}\n",
      "Losses {'ner': 8.24072732831852e-13}\n",
      "Losses {'ner': 2.8304726956594634e-05}\n",
      "Losses {'ner': 2.8304908982698883e-05}\n",
      "Losses {'ner': 0.0005405261087645061}\n",
      "Losses {'ner': 0.0005405261088025389}\n",
      "Losses {'ner': 5.061245687167518e-12}\n",
      "Losses {'ner': 0.22654317327286064}\n",
      "Losses {'ner': 0.22654320854546273}\n",
      "Losses {'ner': 0.22654320886784704}\n",
      "Losses {'ner': 0.22654320907226896}\n",
      "Losses {'ner': 7.299939384831105e-14}\n",
      "Losses {'ner': 8.877878565297609e-09}\n",
      "Losses {'ner': 0.6987306568344703}\n",
      "Losses {'ner': 0.6987306643029287}\n",
      "Losses {'ner': 0.6987348950126209}\n",
      "Losses {'ner': 0.031769092092851156}\n",
      "Losses {'ner': 1.8181150028562552}\n",
      "Losses {'ner': 1.8181150033644276}\n",
      "Losses {'ner': 1.8181150052602313}\n",
      "Losses {'ner': 1.8181150052642154}\n",
      "Losses {'ner': 1.5149311798060454e-10}\n",
      "Losses {'ner': 6.386435277901992e-08}\n",
      "Losses {'ner': 9.617917558746212e-06}\n",
      "Losses {'ner': 9.617917630882372e-06}\n",
      "Losses {'ner': 9.645775535297514e-06}\n",
      "Losses {'ner': 0.0023389332009970303}\n",
      "Losses {'ner': 0.002338996891743582}\n",
      "Losses {'ner': 0.00233904481010582}\n",
      "Losses {'ner': 0.0023390796982940055}\n",
      "Losses {'ner': 0.0023558934860250223}\n",
      "Losses {'ner': 4.3366119814301144e-07}\n",
      "Losses {'ner': 4.93073562441398e-07}\n",
      "Losses {'ner': 0.11449392279850566}\n",
      "Losses {'ner': 0.11449420274316681}\n",
      "Losses {'ner': 0.11449420989701209}\n",
      "Losses {'ner': 0.04283119738416954}\n",
      "Losses {'ner': 0.0428316066101354}\n",
      "Losses {'ner': 0.04283161239542254}\n",
      "Losses {'ner': 0.042906691726396065}\n",
      "Losses {'ner': 0.04290669172639612}\n",
      "Losses {'ner': 7.763811948810436e-09}\n",
      "Losses {'ner': 8.039621668863342e-09}\n",
      "Losses {'ner': 7.214945378455002e-06}\n",
      "Losses {'ner': 0.006601351823206362}\n",
      "Losses {'ner': 0.006601490142883902}\n",
      "Losses {'ner': 4.345710698987826e-12}\n",
      "Losses {'ner': 1.252381557532977e-09}\n",
      "Losses {'ner': 4.085278584090847e-07}\n",
      "Losses {'ner': 1.0624863593037297e-06}\n",
      "Losses {'ner': 1.0626314467620346e-06}\n",
      "Losses {'ner': 1.8386626771296166e-14}\n",
      "Losses {'ner': 2.6638594668086974e-05}\n",
      "Losses {'ner': 2.6638634507436245e-05}\n",
      "Losses {'ner': 2.6689248555791778e-05}\n",
      "Losses {'ner': 2.6689254669260003e-05}\n"
     ]
    }
   ],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 60 iterations\n",
    "  for iteration in range(60):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=4)\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        \n",
    "        example = []\n",
    "        # Update the model with iterating each text\n",
    "        for i in range(len(texts)):\n",
    "            doc = nlp.make_doc(texts[i])\n",
    "            example.append(Example.from_dict(doc, annotations[i]))\n",
    "            \n",
    "        nlp.update(\n",
    "                    example,\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5078c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Zomato', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I ordered from Zomato\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "576daa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Jabong', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I shopped from Jabong\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b6b14d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA_FOOD =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3720317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e809ba11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-ORG',\n",
       " 'B-DATE',\n",
       " 'B-PERSON',\n",
       " 'B-GPE',\n",
       " 'B-MONEY',\n",
       " 'B-CARDINAL',\n",
       " 'B-NORP',\n",
       " 'B-PERCENT',\n",
       " 'B-WORK_OF_ART',\n",
       " 'B-LOC',\n",
       " 'B-TIME',\n",
       " 'B-QUANTITY',\n",
       " 'B-FAC',\n",
       " 'B-EVENT',\n",
       " 'B-ORDINAL',\n",
       " 'B-PRODUCT',\n",
       " 'B-LAW',\n",
       " 'B-LANGUAGE',\n",
       " 'I-ORG',\n",
       " 'I-DATE',\n",
       " 'I-PERSON',\n",
       " 'I-GPE',\n",
       " 'I-MONEY',\n",
       " 'I-CARDINAL',\n",
       " 'I-NORP',\n",
       " 'I-PERCENT',\n",
       " 'I-WORK_OF_ART',\n",
       " 'I-LOC',\n",
       " 'I-TIME',\n",
       " 'I-QUANTITY',\n",
       " 'I-FAC',\n",
       " 'I-EVENT',\n",
       " 'I-ORDINAL',\n",
       " 'I-PRODUCT',\n",
       " 'I-LAW',\n",
       " 'I-LANGUAGE',\n",
       " 'L-ORG',\n",
       " 'L-DATE',\n",
       " 'L-PERSON',\n",
       " 'L-GPE',\n",
       " 'L-MONEY',\n",
       " 'L-CARDINAL',\n",
       " 'L-NORP',\n",
       " 'L-PERCENT',\n",
       " 'L-WORK_OF_ART',\n",
       " 'L-LOC',\n",
       " 'L-TIME',\n",
       " 'L-QUANTITY',\n",
       " 'L-FAC',\n",
       " 'L-EVENT',\n",
       " 'L-ORDINAL',\n",
       " 'L-PRODUCT',\n",
       " 'L-LAW',\n",
       " 'L-LANGUAGE',\n",
       " 'U-ORG',\n",
       " 'U-DATE',\n",
       " 'U-PERSON',\n",
       " 'U-GPE',\n",
       " 'U-MONEY',\n",
       " 'U-CARDINAL',\n",
       " 'U-NORP',\n",
       " 'U-PERCENT',\n",
       " 'U-WORK_OF_ART',\n",
       " 'U-LOC',\n",
       " 'U-TIME',\n",
       " 'U-QUANTITY',\n",
       " 'U-FAC',\n",
       " 'U-EVENT',\n",
       " 'U-ORDINAL',\n",
       " 'U-PRODUCT',\n",
       " 'U-LAW',\n",
       " 'U-LANGUAGE',\n",
       " 'O',\n",
       " 'B-FOOD',\n",
       " 'I-FOOD',\n",
       " 'L-FOOD',\n",
       " 'U-FOOD']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "808e14e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 9.470721395758419}\n",
      "Losses {'ner': 15.116900490838946}\n",
      "Losses {'ner': 24.415624789509558}\n",
      "Losses {'ner': 27.82589823924511}\n",
      "Losses {'ner': 8.357137881296808}\n",
      "Losses {'ner': 16.77019742989401}\n",
      "Losses {'ner': 24.047314184101374}\n",
      "Losses {'ner': 27.983054201603732}\n",
      "Losses {'ner': 6.854073539430679}\n",
      "Losses {'ner': 14.84074332162863}\n",
      "Losses {'ner': 20.618383642673653}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csgrad/souvikda/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:141: UserWarning: [W030] Some entities could not be aligned in the text \"China's noodles are very famous\" with entities \"[(8, 14, 'FOOD')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 23.80365671992468}\n",
      "Losses {'ner': 5.777683388577158}\n",
      "Losses {'ner': 9.870172773825995}\n",
      "Losses {'ner': 16.335179418484476}\n",
      "Losses {'ner': 20.171749048807023}\n",
      "Losses {'ner': 5.712634564896041}\n",
      "Losses {'ner': 11.390115498886479}\n",
      "Losses {'ner': 18.52881988981495}\n",
      "Losses {'ner': 20.519213548402657}\n",
      "Losses {'ner': 5.2318607142787865}\n",
      "Losses {'ner': 8.418960345788662}\n",
      "Losses {'ner': 13.47949066353296}\n",
      "Losses {'ner': 16.8269802064246}\n",
      "Losses {'ner': 4.769700034416964}\n",
      "Losses {'ner': 9.43965425789016}\n",
      "Losses {'ner': 15.396772454065475}\n",
      "Losses {'ner': 16.430396144790613}\n",
      "Losses {'ner': 3.3091682867434873}\n",
      "Losses {'ner': 6.741641553786211}\n",
      "Losses {'ner': 10.436249397918617}\n",
      "Losses {'ner': 12.030930401176699}\n",
      "Losses {'ner': 1.879139140873832}\n",
      "Losses {'ner': 3.9540508809990627}\n",
      "Losses {'ner': 6.298155800989576}\n",
      "Losses {'ner': 8.562480120884999}\n",
      "Losses {'ner': 1.2312045203003095}\n",
      "Losses {'ner': 3.62330817513795}\n",
      "Losses {'ner': 5.684265430497831}\n",
      "Losses {'ner': 6.031661459041501}\n",
      "Losses {'ner': 1.630113088108473}\n",
      "Losses {'ner': 3.288366698751915}\n",
      "Losses {'ner': 3.882975143546005}\n",
      "Losses {'ner': 3.9173072448765214}\n",
      "Losses {'ner': 2.500698395429481}\n",
      "Losses {'ner': 4.3973617880067195}\n",
      "Losses {'ner': 4.419322581722383}\n",
      "Losses {'ner': 5.741769369976008}\n",
      "Losses {'ner': 1.499978367990309}\n",
      "Losses {'ner': 2.364184845561695}\n",
      "Losses {'ner': 3.197022954242108}\n",
      "Losses {'ner': 3.1972122279356583}\n",
      "Losses {'ner': 0.008094423546509771}\n",
      "Losses {'ner': 1.6238069397726256}\n",
      "Losses {'ner': 1.669630923849765}\n",
      "Losses {'ner': 1.6825031072921233}\n",
      "Losses {'ner': 1.3386843243166466}\n",
      "Losses {'ner': 1.3516062816152017}\n",
      "Losses {'ner': 1.3543672721841196}\n",
      "Losses {'ner': 1.393274832593313}\n",
      "Losses {'ner': 0.0038480904255575165}\n",
      "Losses {'ner': 1.8167162817530202}\n",
      "Losses {'ner': 2.0739315735187205}\n",
      "Losses {'ner': 2.0740006338721964}\n",
      "Losses {'ner': 0.00025611563315913743}\n",
      "Losses {'ner': 0.25527240417346575}\n",
      "Losses {'ner': 0.25531264838677237}\n",
      "Losses {'ner': 1.3073586551002374}\n",
      "Losses {'ner': 0.009249562076360835}\n",
      "Losses {'ner': 0.047523673688944605}\n",
      "Losses {'ner': 0.04756366861454508}\n",
      "Losses {'ner': 1.392720127276938}\n",
      "Losses {'ner': 1.131782708977571}\n",
      "Losses {'ner': 1.1317854028164425}\n",
      "Losses {'ner': 1.1425323989585783}\n",
      "Losses {'ner': 1.1427004890531374}\n",
      "Losses {'ner': 1.6137303961996643}\n",
      "Losses {'ner': 2.96751984771833}\n",
      "Losses {'ner': 2.967645509387413}\n",
      "Losses {'ner': 2.9676578529157465}\n",
      "Losses {'ner': 1.4530429162148683e-05}\n",
      "Losses {'ner': 0.8613763102126658}\n",
      "Losses {'ner': 0.8616673027716801}\n",
      "Losses {'ner': 0.8633804301667062}\n",
      "Losses {'ner': 1.1692305814850772}\n",
      "Losses {'ner': 1.1692317426776149}\n",
      "Losses {'ner': 1.169797167460538}\n",
      "Losses {'ner': 1.180633086490923}\n",
      "Losses {'ner': 0.0018148436402284794}\n",
      "Losses {'ner': 0.7757373710253236}\n",
      "Losses {'ner': 0.7761775850822699}\n",
      "Losses {'ner': 0.7761778763720343}\n",
      "Losses {'ner': 2.3744691487145272e-05}\n",
      "Losses {'ner': 0.9848542441614772}\n",
      "Losses {'ner': 1.059933907704629}\n",
      "Losses {'ner': 1.0630207604651085}\n",
      "Losses {'ner': 1.9877843863517193}\n",
      "Losses {'ner': 1.9877914758446216}\n",
      "Losses {'ner': 1.9933335564239916}\n",
      "Losses {'ner': 1.9933335822353175}\n",
      "Losses {'ner': 2.238032982929128e-05}\n",
      "Losses {'ner': 10.022368519283992}\n",
      "Losses {'ner': 10.022566028768207}\n",
      "Losses {'ner': 10.022569956230548}\n",
      "Losses {'ner': 0.0003163541300694874}\n",
      "Losses {'ner': 1.2888598185910654}\n",
      "Losses {'ner': 1.2893762089748622}\n",
      "Losses {'ner': 1.2893782577764612}\n",
      "Losses {'ner': 6.419956899805696e-06}\n",
      "Losses {'ner': 1.5474927717748377}\n",
      "Losses {'ner': 1.561504859983725}\n",
      "Losses {'ner': 1.5615057050190473}\n",
      "Losses {'ner': 1.1013084165590936}\n",
      "Losses {'ner': 2.781841853404996}\n",
      "Losses {'ner': 2.7818882034657846}\n",
      "Losses {'ner': 2.781888339252453}\n",
      "Losses {'ner': 2.1415584116047523e-06}\n",
      "Losses {'ner': 8.691318567359886}\n",
      "Losses {'ner': 10.336630418403203}\n",
      "Losses {'ner': 10.336630441753744}\n",
      "Losses {'ner': 1.7976532824437637}\n",
      "Losses {'ner': 1.7981181688339856}\n",
      "Losses {'ner': 1.798127361609292}\n",
      "Losses {'ner': 1.7981274402154135}\n",
      "Losses {'ner': 3.5343750270578165e-05}\n",
      "Losses {'ner': 1.2773789542330862}\n",
      "Losses {'ner': 1.2783804444980962}\n",
      "Losses {'ner': 1.28258230343071}\n",
      "Losses {'ner': 1.1480780939049737}\n",
      "Losses {'ner': 1.1480782518439254}\n",
      "Losses {'ner': 1.1480784019160697}\n",
      "Losses {'ner': 2.1644876842502447}\n",
      "Losses {'ner': 0.768435337152916}\n",
      "Losses {'ner': 2.326392757185695}\n",
      "Losses {'ner': 2.3271108582845703}\n",
      "Losses {'ner': 2.327146644896253}\n",
      "Losses {'ner': 1.1087585688177926}\n",
      "Losses {'ner': 1.1087585876871833}\n",
      "Losses {'ner': 3.3955852650532536}\n",
      "Losses {'ner': 3.395585567290275}\n",
      "Losses {'ner': 3.796734628178139e-07}\n",
      "Losses {'ner': 0.5379982705161176}\n",
      "Losses {'ner': 0.5380198696339679}\n",
      "Losses {'ner': 0.5388513001823391}\n",
      "Losses {'ner': 1.9714424424847765}\n",
      "Losses {'ner': 1.9716431268484174}\n",
      "Losses {'ner': 1.9726559336229488}\n",
      "Losses {'ner': 1.9726560206109773}\n",
      "Losses {'ner': 0.03475066662864401}\n",
      "Losses {'ner': 0.03475073466745796}\n",
      "Losses {'ner': 0.034753073384824834}\n",
      "Losses {'ner': 1.107789757457934}\n",
      "Losses {'ner': 8.832978105951617e-07}\n",
      "Losses {'ner': 1.6307756457266972e-05}\n",
      "Losses {'ner': 0.17595367284893956}\n",
      "Losses {'ner': 0.17595426207779558}\n",
      "Losses {'ner': 2.0019741103203885}\n",
      "Losses {'ner': 2.0019758032716815}\n",
      "Losses {'ner': 2.001980641392107}\n",
      "Losses {'ner': 2.003337987945642}\n",
      "Losses {'ner': 0.07964090083432193}\n",
      "Losses {'ner': 0.07964167681565608}\n",
      "Losses {'ner': 1.5256334303008183}\n",
      "Losses {'ner': 1.5256334639329252}\n",
      "Losses {'ner': 9.727496914295041e-08}\n",
      "Losses {'ner': 0.03474449660180857}\n",
      "Losses {'ner': 0.05946333119067216}\n",
      "Losses {'ner': 0.05946338914828395}\n",
      "Losses {'ner': 0.0007353719536830131}\n",
      "Losses {'ner': 0.0007967711622029554}\n",
      "Losses {'ner': 1.5009979678967167}\n",
      "Losses {'ner': 1.5012117525000765}\n",
      "Losses {'ner': 2.462944480218798e-05}\n",
      "Losses {'ner': 0.002990501256200557}\n",
      "Losses {'ner': 0.004208316508461211}\n",
      "Losses {'ner': 0.004208319042138411}\n",
      "Losses {'ner': 1.460389219461487}\n",
      "Losses {'ner': 1.4604342601384166}\n",
      "Losses {'ner': 1.4604343836184437}\n",
      "Losses {'ner': 1.4608493773953017}\n",
      "Losses {'ner': 0.0018607623466112633}\n",
      "Losses {'ner': 0.0019130429882382193}\n",
      "Losses {'ner': 0.0019147131849317689}\n",
      "Losses {'ner': 0.001964301868824421}\n",
      "Losses {'ner': 6.27094770849438e-05}\n",
      "Losses {'ner': 0.0003299457499632828}\n",
      "Losses {'ner': 0.00033020493083844643}\n",
      "Losses {'ner': 0.0003332590841603623}\n",
      "Losses {'ner': 7.952072408637911e-07}\n",
      "Losses {'ner': 0.00022512414436000935}\n",
      "Losses {'ner': 0.0032432084261852525}\n",
      "Losses {'ner': 0.003243208619508773}\n",
      "Losses {'ner': 0.8977929950253962}\n",
      "Losses {'ner': 0.8977940027194918}\n",
      "Losses {'ner': 0.9006621166635608}\n",
      "Losses {'ner': 0.9006674988020106}\n",
      "Losses {'ner': 0.0007857173961639796}\n",
      "Losses {'ner': 0.0007857562261622413}\n",
      "Losses {'ner': 0.0008210049006009285}\n",
      "Losses {'ner': 0.0008210639654665281}\n",
      "Losses {'ner': 1.1111833620626178e-06}\n",
      "Losses {'ner': 1.1479821012389715e-06}\n",
      "Losses {'ner': 4.3089934236235844e-05}\n",
      "Losses {'ner': 4.30901089076012e-05}\n",
      "Losses {'ner': 8.703582638331938e-08}\n",
      "Losses {'ner': 2.2598361618225572e-06}\n",
      "Losses {'ner': 2.4370293599555023e-06}\n",
      "Losses {'ner': 2.4373230664283954e-06}\n",
      "Losses {'ner': 1.0221380224152439e-07}\n",
      "Losses {'ner': 1.2977948630796549e-07}\n",
      "Losses {'ner': 8.50991756959194e-06}\n",
      "Losses {'ner': 2.671338200424298e-05}\n",
      "Losses {'ner': 1.1047053660078548e-07}\n",
      "Losses {'ner': 0.022418189358444997}\n",
      "Losses {'ner': 0.2821549778123998}\n",
      "Losses {'ner': 0.28215498614708423}\n",
      "Losses {'ner': 4.175087360202976e-06}\n",
      "Losses {'ner': 0.007791910092606297}\n",
      "Losses {'ner': 0.008417520051555123}\n",
      "Losses {'ner': 0.008417630755410044}\n",
      "Losses {'ner': 0.0003226481847717677}\n",
      "Losses {'ner': 0.0003261262755794148}\n",
      "Losses {'ner': 0.010173567116434796}\n",
      "Losses {'ner': 0.010173591538089784}\n",
      "Losses {'ner': 7.076739942788431e-09}\n",
      "Losses {'ner': 4.4278763290616286e-08}\n",
      "Losses {'ner': 3.6280708473137387e-05}\n",
      "Losses {'ner': 3.6280870774628766e-05}\n",
      "Losses {'ner': 9.247666069306134e-07}\n",
      "Losses {'ner': 1.2373829950087287e-06}\n",
      "Losses {'ner': 0.0004493492524937972}\n",
      "Losses {'ner': 0.0004494411742337446}\n",
      "Losses {'ner': 2.3609216287328265e-06}\n",
      "Losses {'ner': 0.002389370716749794}\n",
      "Losses {'ner': 0.0023894413885926676}\n",
      "Losses {'ner': 0.002389443004530618}\n",
      "Losses {'ner': 2.751737281431822e-05}\n",
      "Losses {'ner': 2.7520528538648345e-05}\n",
      "Losses {'ner': 2.7584294725818294e-05}\n",
      "Losses {'ner': 2.7585412414602703e-05}\n"
     ]
    }
   ],
   "source": [
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(60):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA_FOOD)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA_FOOD, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        \n",
    "        example = []\n",
    "        # Update the model with iterating each text\n",
    "        for i in range(len(texts)):\n",
    "            doc = nlp.make_doc(texts[i])\n",
    "            example.append(Example.from_dict(doc, annotations[i]))\n",
    "            \n",
    "        nlp.update(\n",
    "                    example,\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c659c1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maggi FOOD\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "\n",
    "test_text = \"Maggi is a common fast food popular in India \"\n",
    "doc = nlp(test_text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97450145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2afa1f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wraps FOOD\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Do have Wraps in your menu?\"\n",
    "doc = nlp(test_text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5862233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
