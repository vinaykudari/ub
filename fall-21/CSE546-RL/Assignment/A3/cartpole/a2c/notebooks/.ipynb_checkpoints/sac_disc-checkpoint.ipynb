{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8f80603-334e-4f83-a6da-5ab31e109256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add parent directory to path: enable import from parent dir\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from agents.sac_disc import SAC\n",
    "import pybullet_envs\n",
    "import gym_algorithmic\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43bac3fb-1e10-49fc-b633-cca64f349daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1376f37-6637-4eae-85c8-1ebafe1a6f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603f1b52-461d-45a4-87ed-18ff6aa816d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac = SAC(\n",
    "    env=env,\n",
    "    name='lunar_lander_discrete',\n",
    "    input_dim=env.observation_space.shape[0],\n",
    "    log_freq=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aae9d16-1111-45e6-bcf4-d2bde0452e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting experience...\n",
      "0..10..20..30..40..50..60..70..80..90..100..110..120..130..140..150..160..170..180..190..200..210..Episode: 220, Reward: 18.0, Avg. Reward: 22.76, Policy Loss=-0.81\n",
      "Episode: 230, Reward: 11.0, Avg. Reward: 22.3, Policy Loss=-2.6\n",
      "Episode: 240, Reward: 13.0, Avg. Reward: 23.3, Policy Loss=-4.98\n",
      "Episode: 250, Reward: 31.0, Avg. Reward: 22.1, Policy Loss=-7.24\n",
      "Episode: 260, Reward: 13.0, Avg. Reward: 21.3, Policy Loss=-8.41\n",
      "Episode: 270, Reward: 14.0, Avg. Reward: 22.46, Policy Loss=-10.12\n",
      "Episode: 280, Reward: 19.0, Avg. Reward: 22.86, Policy Loss=-11.56\n",
      "Episode: 290, Reward: 15.0, Avg. Reward: 22.44, Policy Loss=-12.66\n",
      "Episode: 300, Reward: 18.0, Avg. Reward: 20.68, Policy Loss=-13.44\n",
      "Episode: 310, Reward: 14.0, Avg. Reward: 21.2, Policy Loss=-14.22\n",
      "Episode: 320, Reward: 17.0, Avg. Reward: 20.42, Policy Loss=-14.97\n",
      "Episode: 330, Reward: 18.0, Avg. Reward: 20.3, Policy Loss=-15.71\n",
      "Episode: 340, Reward: 12.0, Avg. Reward: 21.08, Policy Loss=-16.48\n",
      "Episode: 350, Reward: 31.0, Avg. Reward: 21.6, Policy Loss=-17.04\n",
      "Episode: 360, Reward: 12.0, Avg. Reward: 21.94, Policy Loss=-17.31\n",
      "Episode: 370, Reward: 19.0, Avg. Reward: 22.64, Policy Loss=-17.76\n",
      "Episode: 380, Reward: 23.0, Avg. Reward: 23.12, Policy Loss=-18.06\n",
      "Episode: 390, Reward: 13.0, Avg. Reward: 23.18, Policy Loss=-18.46\n",
      "Episode: 400, Reward: 26.0, Avg. Reward: 22.5, Policy Loss=-18.74\n",
      "Episode: 410, Reward: 28.0, Avg. Reward: 22.52, Policy Loss=-18.86\n",
      "Episode: 420, Reward: 19.0, Avg. Reward: 22.06, Policy Loss=-19.01\n",
      "Episode: 430, Reward: 12.0, Avg. Reward: 23.36, Policy Loss=-19.22\n",
      "Episode: 440, Reward: 19.0, Avg. Reward: 22.28, Policy Loss=-19.26\n",
      "Episode: 450, Reward: 28.0, Avg. Reward: 23.5, Policy Loss=-19.44\n",
      "Episode: 460, Reward: 42.0, Avg. Reward: 23.56, Policy Loss=-19.48\n",
      "Episode: 470, Reward: 22.0, Avg. Reward: 23.48, Policy Loss=-19.38\n",
      "Episode: 480, Reward: 13.0, Avg. Reward: 20.78, Policy Loss=-19.39\n",
      "Episode: 490, Reward: 13.0, Avg. Reward: 21.26, Policy Loss=-19.45\n",
      "Episode: 500, Reward: 13.0, Avg. Reward: 21.6, Policy Loss=-19.49\n",
      "Episode: 510, Reward: 22.0, Avg. Reward: 21.24, Policy Loss=-19.47\n",
      "Episode: 520, Reward: 17.0, Avg. Reward: 20.72, Policy Loss=-19.4\n",
      "Episode: 530, Reward: 11.0, Avg. Reward: 21.62, Policy Loss=-19.41\n",
      "Episode: 540, Reward: 30.0, Avg. Reward: 21.12, Policy Loss=-19.35\n",
      "Episode: 550, Reward: 10.0, Avg. Reward: 19.32, Policy Loss=-19.31\n",
      "Episode: 560, Reward: 28.0, Avg. Reward: 20.1, Policy Loss=-19.33\n",
      "Episode: 570, Reward: 17.0, Avg. Reward: 19.94, Policy Loss=-19.19\n",
      "Episode: 580, Reward: 21.0, Avg. Reward: 20.74, Policy Loss=-19.18\n",
      "Episode: 590, Reward: 15.0, Avg. Reward: 22.16, Policy Loss=-19.25\n",
      "Episode: 600, Reward: 36.0, Avg. Reward: 23.28, Policy Loss=-19.19\n",
      "Episode: 610, Reward: 14.0, Avg. Reward: 22.48, Policy Loss=-19.12\n",
      "Episode: 620, Reward: 23.0, Avg. Reward: 22.76, Policy Loss=-19.05\n",
      "Episode: 630, Reward: 12.0, Avg. Reward: 20.88, Policy Loss=-18.92\n",
      "Episode: 640, Reward: 47.0, Avg. Reward: 21.66, Policy Loss=-18.91\n",
      "Episode: 650, Reward: 35.0, Avg. Reward: 21.3, Policy Loss=-18.85\n",
      "Episode: 660, Reward: 40.0, Avg. Reward: 21.56, Policy Loss=-18.77\n",
      "Episode: 670, Reward: 13.0, Avg. Reward: 21.76, Policy Loss=-18.73\n",
      "Episode: 680, Reward: 23.0, Avg. Reward: 24.14, Policy Loss=-18.66\n",
      "Episode: 690, Reward: 11.0, Avg. Reward: 21.46, Policy Loss=-18.57\n",
      "Episode: 700, Reward: 36.0, Avg. Reward: 22.08, Policy Loss=-18.57\n",
      "Episode: 710, Reward: 23.0, Avg. Reward: 22.94, Policy Loss=-18.56\n",
      "Episode: 720, Reward: 20.0, Avg. Reward: 23.66, Policy Loss=-18.55\n",
      "Episode: 730, Reward: 16.0, Avg. Reward: 23.04, Policy Loss=-18.47\n",
      "Episode: 740, Reward: 42.0, Avg. Reward: 22.6, Policy Loss=-18.5\n",
      "Episode: 750, Reward: 13.0, Avg. Reward: 23.2, Policy Loss=-18.55\n",
      "Episode: 760, Reward: 27.0, Avg. Reward: 24.52, Policy Loss=-18.55\n",
      "Episode: 770, Reward: 14.0, Avg. Reward: 23.8, Policy Loss=-18.55\n",
      "Episode: 780, Reward: 23.0, Avg. Reward: 22.56, Policy Loss=-18.52\n",
      "Episode: 790, Reward: 35.0, Avg. Reward: 24.8, Policy Loss=-18.61\n",
      "Episode: 800, Reward: 44.0, Avg. Reward: 24.42, Policy Loss=-18.69\n",
      "Episode: 810, Reward: 17.0, Avg. Reward: 24.04, Policy Loss=-18.69\n",
      "Episode: 820, Reward: 19.0, Avg. Reward: 24.36, Policy Loss=-18.84\n",
      "Episode: 830, Reward: 19.0, Avg. Reward: 24.64, Policy Loss=-18.82\n",
      "Episode: 840, Reward: 29.0, Avg. Reward: 23.48, Policy Loss=-18.77\n",
      "Episode: 850, Reward: 14.0, Avg. Reward: 22.56, Policy Loss=-18.87\n",
      "Episode: 860, Reward: 17.0, Avg. Reward: 20.32, Policy Loss=-18.83\n",
      "Episode: 870, Reward: 26.0, Avg. Reward: 20.74, Policy Loss=-18.79\n",
      "Episode: 880, Reward: 33.0, Avg. Reward: 22.7, Policy Loss=-18.92\n",
      "Episode: 890, Reward: 14.0, Avg. Reward: 21.24, Policy Loss=-18.86\n",
      "Episode: 900, Reward: 18.0, Avg. Reward: 21.2, Policy Loss=-18.83\n",
      "Episode: 910, Reward: 30.0, Avg. Reward: 21.58, Policy Loss=-18.88\n",
      "Episode: 920, Reward: 33.0, Avg. Reward: 21.0, Policy Loss=-18.79\n",
      "Episode: 930, Reward: 59.0, Avg. Reward: 19.24, Policy Loss=-18.8\n",
      "Episode: 940, Reward: 16.0, Avg. Reward: 20.3, Policy Loss=-18.81\n",
      "Episode: 950, Reward: 13.0, Avg. Reward: 20.96, Policy Loss=-18.79\n",
      "Episode: 960, Reward: 32.0, Avg. Reward: 20.86, Policy Loss=-18.75\n",
      "Episode: 970, Reward: 12.0, Avg. Reward: 20.72, Policy Loss=-18.7\n",
      "Episode: 980, Reward: 18.0, Avg. Reward: 19.98, Policy Loss=-18.62\n",
      "Episode: 990, Reward: 17.0, Avg. Reward: 19.64, Policy Loss=-18.58\n"
     ]
    }
   ],
   "source": [
    "sac.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
