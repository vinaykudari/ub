{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab6fc652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class MonteCarlo:\n",
    "    def __init__(\n",
    "        self, \n",
    "        env,\n",
    "        gamma=0.01,\n",
    "        episodes=1000,\n",
    "        eval_episodes=50,\n",
    "        epsilon_start=0.8,\n",
    "        epsilon_decay=0.9999,\n",
    "        epsilon_min=0.01,\n",
    "        negative_rewards=[-0.75, -0.85, -5.0],\n",
    "        max_eval_timesteps=20,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.n_states = self.env.observation_space.n\n",
    "        self.states = self.env.states\n",
    "        self.n_actions = self.env.action_space.n\n",
    "        self.actions = self.env.actions\n",
    "        self.episodes = episodes\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.eval_episodes = eval_episodes\n",
    "        \n",
    "        # initialize traning logs\n",
    "        self.logs = defaultdict(\n",
    "            lambda: {\n",
    "                'bad_state_count': 0,\n",
    "                'timesteps': 0,\n",
    "                'goal_achieved': False,\n",
    "                'reward': 0,\n",
    "                'cumulative_reward': 0,\n",
    "                'epsilon': None,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        #initialize evaluation logs\n",
    "        self.eval_logs = defaultdict(\n",
    "            lambda: {\n",
    "                'bad_state_count': 0,\n",
    "                'timesteps': 0,\n",
    "                'goal_achieved': False,\n",
    "                'reward': 0,\n",
    "                'cumulative_reward': 0,\n",
    "            },\n",
    "        )\n",
    "        self.negative_rewards = negative_rewards\n",
    "        self.max_eval_timesteps = max_eval_timesteps\n",
    "        \n",
    "        \n",
    "        # initialize random policy\n",
    "        self.policy = {\n",
    "            state:np.random.choice(self.actions) for state in self.states\n",
    "        }\n",
    "    \n",
    "        # initialize action-value function\n",
    "        self.Q = defaultdict(\n",
    "            lambda: np.zeros(self.n_actions),\n",
    "        )\n",
    "        \n",
    "        # initialize state-action counts\n",
    "        self.sa_count = defaultdict(\n",
    "            lambda: np.zeros(self.n_actions),\n",
    "        )\n",
    "        \n",
    "    def _get_action_probs(self, Q_s, epsilon):\n",
    "        # initialize episilon probability to all the actions\n",
    "        probs = np.ones(self.n_actions) * (epsilon / self.n_actions)\n",
    "        best_action = np.argmax(Q_s)\n",
    "        # initialize 1-epsilon probability to the greedy action\n",
    "        probs[best_action] = 1 - epsilon + (epsilon / self.n_actions)\n",
    "        return probs\n",
    "        \n",
    "    def _get_action(self, state, epsilon):\n",
    "        # select action based in epsilon greedy fashion \n",
    "        action = np.random.choice(\n",
    "            self.actions, \n",
    "            p=self._get_action_probs(\n",
    "                self.Q[state],\n",
    "                epsilon,\n",
    "            ),\n",
    "        ) \n",
    "        \n",
    "        return action, self.actions.index(action)\n",
    "    \n",
    "    def _simulate_one_episode(self, n, epsilon):\n",
    "        state = self.env.reset()\n",
    "        episode = []\n",
    "        timesteps = 0\n",
    "        episode_ended = False\n",
    "        \n",
    "        while not episode_ended:\n",
    "            action, a_idx = self._get_action(state, epsilon)\n",
    "            _, reward, goal, current_state, episode_ended = self.env.step(action=action)\n",
    "            \n",
    "            if reward in self.negative_rewards:\n",
    "                self.logs[n]['bad_state_count'] += 1\n",
    "            \n",
    "            # save logs for analysis\n",
    "            self.logs[n]['reward'] += reward\n",
    "            self.logs[n]['cumulative_reward'] = self.logs[n]['reward']\n",
    "            self.logs[n]['goal_achieved'] = goal\n",
    "                \n",
    "            # save timestep information\n",
    "            episode.append((state, a_idx, reward))\n",
    "            state = current_state\n",
    "            timesteps += 1\n",
    "            \n",
    "        self.logs[n]['timesteps'] = timesteps\n",
    "        \n",
    "        return episode\n",
    "    \n",
    "    def _update_Q(self, episode):\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        states_actions = list(zip(states, actions))\n",
    "        G = 0\n",
    "        \n",
    "        for timestep in range(len(episode)-1, -1, -1):\n",
    "            state, action, reward = episode[timestep]\n",
    "            G = reward + self.gamma*G\n",
    "            # one step monte carlo\n",
    "            if (state, action) not in states_actions[:timestep]:\n",
    "                old_Q = self.Q[state][action]\n",
    "                self.sa_count[state][action] += 1\n",
    "                alpha = 1 / self.sa_count[state][action]\n",
    "                self.Q[state][action] = old_Q + alpha * (G - old_Q)\n",
    "                \n",
    "                # update policy using Q value function\n",
    "                self.policy[state] = self.actions[\n",
    "                    np.argmax(self.Q[state])\n",
    "                ]\n",
    "            \n",
    "                \n",
    "    def run(self):\n",
    "        epsilon = self.epsilon_start\n",
    "        for episode_no in range(self.episodes):\n",
    "            # set lower limit for epsilon\n",
    "            epsilon = max(epsilon*self.epsilon_decay, self.epsilon_min)\n",
    "            self.logs[episode_no]['epsilon'] = epsilon\n",
    "            episode = self._simulate_one_episode(episode_no, epsilon)\n",
    "            self._update_Q(episode)\n",
    "            \n",
    "            if episode_no > 0:\n",
    "                self.logs[episode_no]['cumulative_reward'] += \\\n",
    "                self.logs[episode_no-1]['cumulative_reward']\n",
    "            \n",
    "        return self.policy, self.Q\n",
    "    \n",
    "    def evaluate(self, policy=None):\n",
    "        if not policy:\n",
    "            policy = self.policy\n",
    "        \n",
    "        for n in range(self.eval_episodes):\n",
    "            done = False\n",
    "            state = self.env.reset()\n",
    "            timesteps = 0\n",
    "            \n",
    "            while not done:\n",
    "                _, reward, goal, state, done = self.env.step(\n",
    "                    action=self.policy[state],\n",
    "                )\n",
    "                timesteps += 1\n",
    "                \n",
    "                if reward in self.negative_rewards:\n",
    "                    self.eval_logs[n]['bad_state_count'] += 1\n",
    "                    \n",
    "                self.eval_logs[n]['reward'] += reward\n",
    "                self.eval_logs[n]['cumulative_reward'] = self.eval_logs[n]['reward']\n",
    "                self.eval_logs[n]['goal_achieved'] = goal\n",
    "                \n",
    "            self.eval_logs[n]['timesteps'] = timesteps\n",
    "            \n",
    "            if n > 0:\n",
    "                self.eval_logs[n]['cumulative_reward'] += \\\n",
    "                self.eval_logs[n-1]['cumulative_reward']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
